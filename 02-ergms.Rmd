---
title: "02-redes"
author: "George G. Vega Yon"
date: "December 9, 2018"
output: html_document
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Un poco de historia

De @Hunter2006:

There has been a lot of work on models of the form (1.1), to which we refer as expo-
nential random graph models or ERGMs for short. (We avoid the lengthier EFRGM, for
“exponential family random graph models,” both for the sake of brevity and because we
consider some models in this article that should technically be called curved exponential
families.)

*   Holland and Leinhardt (1981) appear to be the first to propose a specific case of
model (1.1) in the literature. Their model, which they called the p1 model, resulted in each
dyad—by which we mean each pair of nodes—having edges independently of every other
dyad.

*   Based on developments in spatial statistics (Besag 1974), Frank and Strauss (1986)
generalized to the case in which dyads exhibit a kind of Markovian dependence: Two dyads
are dependent, conditional on the rest of the graph, only when they share a node.

*   Frank (1991) mentioned the application of model (1.1) to social networks in its full generality.

*   This was pursued by Wasserman and Pattison (1996). In honor of Holland and Leinhardt’s
p1 model, they referred to model (1.1) as p∗ (p-star), a name that has been widely applied
to ERGMs in the social networks literature.


## Exponential Random Graph Models (ERGMs)

*   ¿Qué conjunto de sub-estructuras dan origen al grafo observado?

*   Por ejemplo: \# de triadas, \# de diadas homofílicas, \# de k-estrellas, etc.

TABLA VA ACA


*   En este caso, $\mathbf{Y} = \mathbf{G}$

*   En general, la distribución de $\mathbf{Y}$ puede ser paremetrizada como sigue:
    
    $$
    \Pr\left(\mathbf{Y}=\mathbf{y}|\theta, \mathcal{Y}\right) = \frac{\exp{\theta^{\mbox{T}}\mathbf{g}(\mathbf{y})}}{\kappa\left(\theta, \mathcal{Y}\right)},\quad\mathbf{y}\in\mathcal{Y}
    \tag{1}
    $$
    
    Donde $\theta\in\Omega\subset\mathbb{R}^q$ es el vactor de coeficientes, y 
    $\mathbf{g}(\mathbf{y})$ es un *q*-vector de estadísticos basados en el
    grafo $\mathbf{y}$.

---

*   *Markov dependence* lleva a *Markov Graphs* [@Frank1986].

*   Intro a ERGMs [@ROBINS2007a; @ROBINS2007b].

*   El modelo (1) puede ser extendido para incluir covariables (*features*) 
    a nivel nodal o de lazo reemplazando $\mathbf{g}(\mathbf{y})$ por
    $\mathbf{g}(\mathbf{y}, \mathbf{X})$ 
    
*   El denominador,
    
    
    $$
    \kappa\left(\theta,\mathcal{Y}\right) = \sum_{\mathbf{z}\in\mathcal{Y}}\exp{\theta^{\mbox{T}}\mathbf{g}(\mathbf{z})}
    $$
    0
    
    Es la constante de normalización que asegura que (1) es una funcion de probabilidad.

*   Aun restringiendo el conjunto $\mathcal{Y}$ a las redes de tamaño $n$,
    el numero de elementos de $\mathcal{Y}$ hace la estimación de estos modelos
    difícil [@Hunter2008].
    
    $$
    N = 2^{n(n-1)}
    $$
    
## ERGMs: Principales supuestos

*   Modelo generador de datos

*   Dependencia de diadas

*   Red completa

*   Lado práctico: Red relativemente mediana (par de miles de nodos, no más que eso)


## Separable Exponential Random Graph Models (a.k.a. TERGMs)

-   A discrete time model.

-   Estimates a set of parameters $\theta = \{\theta^-, \theta^+\}$ that capture the transition dynamics from $\mathbf{Y}^{t-1}$ to $\mathbf{Y}^{t}$.

-   Assuming that $(\mathbf{Y}^+\perp\mathbf{Y}^-) | \mathbf{Y}^{t-1}$ (the model **dynamic model** is separable), we estimate two models:
    $$
    \begin{align}
    \Pr\left(\mathbf{Y}^+ = \mathbf{y}^+|\mathbf{Y}^{t-1} = \mathbf{y}^{t-1};\theta^+\right),\quad \mathbf{y}^+\in\mathcal{Y}^+(\mathbf{y}^{t-1})\\
    \Pr\left(\mathbf{Y}^- = \mathbf{y}^-|\mathbf{Y}^{t-1} = \mathbf{y}^{t-1};\theta^-\right),\quad \mathbf{y}^-\in\mathcal{Y}^-(\mathbf{y}^{t-1})
    \end{align}
    $$

-   So we end up estimating two ERGMs.
    
## Latent Network Models

-   Social networks are a function of a latent space (literally, xyz for example) $\mathbf{Z}$.

-   Individuals who are closer to each other within $\mathbf{Z}$ have a higher
chance of been connected.

-   Besides of estimating the typical set of parameters $\theta$, a key part of this model is find $\mathbf{Z}$.

-   Similar to TERGMs, under the conditional independence assumption we can estimate:

$$
\Pr\left(\mathbf{Y} =\mathbf{y}|\mathbf{X} = \mathbf{x}, \mathbf{Z}, \theta\right) = \prod_{i\neq j}\Pr\left(y_{ij}|z_i, z_j, x_{ij},\theta\right)
$$

See @hoff2002



## ERGMs: Estimación

*   En el caso general, la estimacion de esta familia de modelos no es factible
    utilizando enumeración exahustiva de los grafos.
    
*   Por ejemplo, si $n = 7$, entonces $\left|\mathcal{Y}\right| = 2^{7(7-1)} \approx 4.39\times10^{12}$

*   Varios métodos propuestos para resolver este problema: Algoritmo Robbins-Monro
    (*stochastic approximation*, @robbins1951), MCMC-MLE

*   Revisaremos el último

---

### MCMC-MLE

1.  Inicializar el algoritmo con un $\theta=\theta^{(t)}$,
    por lo general se utiliza *Maximum-Pseudo-Likelihood-Estimator* (MPLE) que
    equivale a una regresión logística.

2.  Mientras $\neg{}Converencia$ :
    
    a.  Utilizando $\theta^{(t)}$, simula $M$ redes modificando el grafo
        $\mathbf{Y}_{obs}$ link a link. Esta parte se realiza utilizando un algoritmo
        de *importance-sampling* donde la ponderacion de cada grafo esta dada por
        la verosimilityd condicional en $\theta^{(t)}$
    
    b.  Con las redes simuladas, podemos realizar el paso *Newton* para actualizar
        $\theta^{(t)}$ (esta es la parte de iteración en el paquete `ergm`):
        $\theta^{(t)}\to\theta^{(t+1)}$
    
    c.  De haber alcanzado convergencia (Ej: la distancia entre $\theta^{(t)}$ y
        $\theta^{(t + 1)}$ es cercana a cero), detener, de lo contrario volver
        al paso a.

Para más detalles vea @lusher2012; @admiraal2006; @Snijders2002; @Wang2009
da detalles respecto del algoritmo utilizado en PNet (Robbins-Monro en `RSiena`).
@lusher2012 muestra de manera breve las diferencias entre `ergm` and `PNet`. 

-----

*   *Degeneracy* @handcock2003; @Schweinberger2015


## Context  (cont'd)

\begin{figure}
\centering
\includegraphics[width = .65\linewidth]{plot-graph-4-1.pdf}
\end{figure}

\pause

How can we go beyond descriptive statistics? 

## Redes pequeñas y ERGMs

Cuando intentamos estimar ERGMs en redes pequeñas: \pause

*   MCMC no converge al intentar estimar un modelo de tipo *block diagonal*
    (zeros estructurales),\pause

*   Lo mismo ocurre cuando intentamos estimar un ERGM para una red pequeña, \pause

*   ~~Even if it converges, the Asymptotic properties of MLEs are no longer valid
    since the sample size is not large enough.~~ (not true! ver ASSSASS)

    
## Repensando el problema

\pause

*   1st Step: Olvidemos el MCMC-MLE, aprovechemos el tamaño de la red para calcular
    la funcion de log-verosimilitud y apliquemos MLE: \pause
    
    $$
    \Pr\left(\mathbf{Y}=\mathbf{y}|\theta, \mathcal{Y}\right) = \frac{\exp{\theta^{\mbox{T}}\mathbf{g}(\mathbf{y})}}{\kappa\left(\theta, \mathcal{Y}\right)},\quad\mathbf{y}\in\mathcal{Y}
    $$
    
*   Esto resuelve el problam de *degenarcy* encontrado en ERGMs. \pause

*   Lo que ya esta implementado en el paquete `lergm`
    (disponible en https://github.com/USCCANA/lergm):\pause
    
    *   No desde 0: Utiliza algunas de las funciones ya incluidas en el paquete
        de statnet, `ergm`, en particular
        `ergm-terms`, `formula_ergm`.\pause
    
    *   De alto rendimiento (hasta un punto): Algunos de sus componentes están
        escritos en C++\pause
    
    *   Estapa experimental...\pause ~~we'll see if it is worth keep
        working on it!~~

## Ejemplo 1

Partamos por estimar un ERGM para una red con 4 nodos

```{r lergm1, echo=TRUE}
library(lergm)
set.seed(12)
x <- sna::rgraph(4)
lergm(x ~ edges + balance + mutual)
```

A esto le llamo ERGMito (Gracias a George Barnett en la conferencia NASN2018!)

## Pooled ERGMitos

*   Cuando estimamos el modelo de bloque diagonal, en cierta medida estamos asumiendo
    que las redes son idependientes.\pause

*   Esto implica que podemos hacer lo mismo con la función de verosimilitud, en
    otras palabras, la verosimilitud conjunta es el producto de las independientes:\pause
    
    $$
    \Pr\left(\mathbf{Y}=\{\mathbf{y}_{\color{cyan} i}\}|\theta, \left\{\mathcal{Y}_{\color{cyan}i}\right\}\right) = {\color{cyan} \prod_i} \frac{\exp{\theta^{\mbox{T}}\mathbf{g}(\mathbf{y}_{\color{cyan} i})}}{\kappa_{\color{cyan} i}\left(\theta, \mathcal{Y}_{\color{cyan}i}\right)}
    $$
    \pause
    
*   Estimando una version *pooled* del ERGMito mejoramos la precisión de nuestro
    estimador.\pause
    
*   Esto también esta en el paquete `lergm`
    
## Ejemplo 2

Supon que tenemos 3 grafos pequeños de tamaño 4, 5, y 5 respectivamente:

```{r lergm2, echo=TRUE}
library(lergm)
set.seed(12)
x1 <- sna::rgraph(4)
x2 <- sna::rgraph(5)
x3 <- sna::rgraph(5)

lergm(list(x1, x2, x3) ~ edges + balance + mutual)
```


# Referencias

